{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8af734b-25c7-4041-bf10-fd31ab03ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Downloading data if needed...\n",
      "data_repo\\train.txt exists, skipping download.\n",
      "data_repo\\val.txt exists, skipping download.\n",
      "data_repo\\test.txt exists, skipping download.\n",
      "Dataset sizes -> train: 213793 val: 7176 test: 7167\n",
      "Token coords (first 8):\n",
      "    a idx= 0 -> [-0.25091976  0.9014286 ]\n",
      "    b idx= 1 -> [0.4639879  0.19731697]\n",
      "    c idx= 2 -> [-0.6879627  -0.68801093]\n",
      "    d idx= 3 -> [-0.88383275  0.7323523 ]\n",
      "    e idx= 4 -> [0.20223002 0.41614518]\n",
      "    f idx= 5 -> [-0.958831   0.9398197]\n",
      "    g idx= 6 -> [ 0.6648853 -0.5753218]\n",
      "    h idx= 7 -> [-0.63635004 -0.633191  ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instanced. Total params: 8,336, Trainable: 8,336\n",
      "\n",
      "Token embedding (first 6 rows):\n",
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "         -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
      "        [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,  1.6806,\n",
      "          1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,  0.8599],\n",
      "        [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057, -0.7746,\n",
      "         -1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347, -0.4879],\n",
      "        [-0.9138, -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360,\n",
      "         -1.4032,  0.0360, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,  1.3835],\n",
      "        [ 1.4451,  0.8564,  2.2181,  0.5232,  0.3466, -0.1973, -1.0546,  1.2780,\n",
      "         -0.1722,  0.5238,  0.0566,  0.4263,  0.5750, -0.6417, -2.2064, -0.7508],\n",
      "        [ 0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,\n",
      "          0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835]])\n",
      "\n",
      "raw_channel_weights (before softmax): tensor([1., 1., 1.])\n",
      "channel weights (softmax): [0.33333334 0.33333334 0.33333334]\n",
      "\n",
      "Model architecture:\n",
      "ThreeChannelPredictor(\n",
      "  (tok_embed): Embedding(27, 16)\n",
      "  (ch1_mlp): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      "  (ch2_mlp): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      "  (ch3_mlp): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=27, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Parameters (name, shape, #params, trainable):\n",
      " - raw_channel_weights                      | shape (3,)                 | #      3 | trainable=True\n",
      " - tok_embed.weight                         | shape (27, 16)             | #    432 | trainable=True\n",
      " - ch1_mlp.0.weight                         | shape (64, 6)              | #    384 | trainable=True\n",
      " - ch1_mlp.0.bias                           | shape (64,)                | #     64 | trainable=True\n",
      " - ch1_mlp.2.weight                         | shape (16, 64)             | #   1024 | trainable=True\n",
      " - ch1_mlp.2.bias                           | shape (16,)                | #     16 | trainable=True\n",
      " - ch1_mlp.4.weight                         | shape (1, 16)              | #     16 | trainable=True\n",
      " - ch1_mlp.4.bias                           | shape (1,)                 | #      1 | trainable=True\n",
      " - ch2_mlp.0.weight                         | shape (64, 6)              | #    384 | trainable=True\n",
      " - ch2_mlp.0.bias                           | shape (64,)                | #     64 | trainable=True\n",
      " - ch2_mlp.2.weight                         | shape (16, 64)             | #   1024 | trainable=True\n",
      " - ch2_mlp.2.bias                           | shape (16,)                | #     16 | trainable=True\n",
      " - ch2_mlp.4.weight                         | shape (1, 16)              | #     16 | trainable=True\n",
      " - ch2_mlp.4.bias                           | shape (1,)                 | #      1 | trainable=True\n",
      " - ch3_mlp.0.weight                         | shape (64, 48)             | #   3072 | trainable=True\n",
      " - ch3_mlp.0.bias                           | shape (64,)                | #     64 | trainable=True\n",
      " - ch3_mlp.2.weight                         | shape (27, 64)             | #   1728 | trainable=True\n",
      " - ch3_mlp.2.bias                           | shape (27,)                | #     27 | trainable=True\n",
      "\n",
      "Starting training loop (steps = 50000)\n",
      "step      0/50000 | tot_loss 6.6298 | ce_chs 7.0801,3.9496,3.2699 | ang_mse 3.445246 dist_mse 0.281373 | accs 0.062,0.016,0.031 | w=[0.33333334 0.33333334 0.33333334] | lr=1.000e-07 | elapsed=0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdhome\\AppData\\Local\\Temp\\ipykernel_10808\\815072528.py:288: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t0 = torch.tensor(batch[\"t0\"], dtype=torch.long, device=DEVICE)\n",
      "C:\\Users\\kdhome\\AppData\\Local\\Temp\\ipykernel_10808\\815072528.py:289: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t1 = torch.tensor(batch[\"t1\"], dtype=torch.long, device=DEVICE)\n",
      "C:\\Users\\kdhome\\AppData\\Local\\Temp\\ipykernel_10808\\815072528.py:290: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t2 = torch.tensor(batch[\"t2\"], dtype=torch.long, device=DEVICE)\n",
      "C:\\Users\\kdhome\\AppData\\Local\\Temp\\ipykernel_10808\\815072528.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t3 = torch.tensor(batch[\"t3\"], dtype=torch.long, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   1000/50000 | tot_loss 4.8768 | ce_chs 5.9109,3.5703,2.7464 | ang_mse 1.513063 dist_mse 0.206732 | accs 0.047,0.047,0.188 | w=[0.3124529  0.34190607 0.345641  ] | lr=1.000e-04 | elapsed=17.9s\n",
      "step   2000/50000 | tot_loss 5.2091 | ce_chs 6.1983,3.5604,2.4410 | ang_mse 2.451267 dist_mse 0.211952 | accs 0.031,0.078,0.297 | w=[0.27858156 0.34821454 0.37320393] | lr=9.991e-05 | elapsed=33.1s\n",
      "step   3000/50000 | tot_loss 4.6428 | ce_chs 5.4322,3.8385,2.4652 | ang_mse 1.662568 dist_mse 0.260890 | accs 0.016,0.047,0.297 | w=[0.2500814  0.34503686 0.40488175] | lr=9.963e-05 | elapsed=48.0s\n",
      "step   4000/50000 | tot_loss 4.9890 | ce_chs 5.4978,3.7347,2.3680 | ang_mse 2.687830 dist_mse 0.235913 | accs 0.047,0.047,0.312 | w=[0.22562209 0.3314152  0.44296277] | lr=9.917e-05 | elapsed=68.0s\n",
      "step   5000/50000 | tot_loss 4.7681 | ce_chs 5.6195,3.6223,2.4457 | ang_mse 2.404229 dist_mse 0.213200 | accs 0.016,0.062,0.156 | w=[0.20528902 0.30780262 0.4869084 ] | lr=9.853e-05 | elapsed=88.3s\n",
      "step   6000/50000 | tot_loss 4.9999 | ce_chs 6.4244,3.7552,2.4525 | ang_mse 2.647962 dist_mse 0.238030 | accs 0.016,0.031,0.250 | w=[0.18675092 0.27842143 0.5348277 ] | lr=9.771e-05 | elapsed=105.7s\n",
      "step   7000/50000 | tot_loss 5.8246 | ce_chs 7.1311,3.6899,2.5461 | ang_mse 4.208332 dist_mse 0.233841 | accs 0.000,0.109,0.250 | w=[0.1684445  0.24927445 0.5822811 ] | lr=9.671e-05 | elapsed=121.3s\n",
      "step   8000/50000 | tot_loss 4.3147 | ce_chs 5.5259,3.5484,2.1923 | ang_mse 2.435205 dist_mse 0.199895 | accs 0.062,0.078,0.344 | w=[0.15123427 0.22169703 0.6270687 ] | lr=9.554e-05 | elapsed=138.9s\n",
      "step   9000/50000 | tot_loss 5.1982 | ce_chs 6.9411,3.5749,2.3967 | ang_mse 3.714510 dist_mse 0.199194 | accs 0.031,0.109,0.234 | w=[0.13496193 0.19631343 0.6687246 ] | lr=9.421e-05 | elapsed=157.3s\n",
      "step  10000/50000 | tot_loss 3.7183 | ce_chs 5.3372,3.5577,2.3137 | ang_mse 1.461668 dist_mse 0.191409 | accs 0.031,0.062,0.234 | w=[0.11991357 0.1732779  0.70680857] | lr=9.271e-05 | elapsed=172.3s\n",
      "step  11000/50000 | tot_loss 3.9422 | ce_chs 4.8377,3.6297,2.1890 | ang_mse 2.284789 dist_mse 0.218026 | accs 0.078,0.125,0.250 | w=[0.10631864 0.15280914 0.7408722 ] | lr=9.106e-05 | elapsed=190.7s\n",
      "step  12000/50000 | tot_loss 4.4080 | ce_chs 5.4963,3.6549,2.7491 | ang_mse 2.333746 dist_mse 0.223719 | accs 0.078,0.094,0.172 | w=[0.09406696 0.13447575 0.7714573 ] | lr=8.926e-05 | elapsed=207.4s\n",
      "step  13000/50000 | tot_loss 3.9882 | ce_chs 6.1599,3.7075,2.4929 | ang_mse 1.859643 dist_mse 0.234179 | accs 0.000,0.000,0.234 | w=[0.0830517  0.11848786 0.7984605 ] | lr=8.732e-05 | elapsed=224.3s\n",
      "step  14000/50000 | tot_loss 3.7277 | ce_chs 5.9788,3.3688,2.1116 | ang_mse 2.248781 dist_mse 0.153445 | accs 0.031,0.047,0.281 | w=[0.07336195 0.10441124 0.8222268 ] | lr=8.525e-05 | elapsed=241.5s\n",
      "step  15000/50000 | tot_loss 4.0850 | ce_chs 6.3162,3.6942,2.5961 | ang_mse 2.062512 dist_mse 0.230115 | accs 0.016,0.031,0.219 | w=[0.06491393 0.09208118 0.8430048 ] | lr=8.305e-05 | elapsed=256.7s\n",
      "step  16000/50000 | tot_loss 3.6947 | ce_chs 5.4868,3.6527,2.4143 | ang_mse 1.792420 dist_mse 0.213611 | accs 0.031,0.062,0.328 | w=[0.05748602 0.08138627 0.8611277 ] | lr=8.074e-05 | elapsed=280.7s\n",
      "step  17000/50000 | tot_loss 4.1589 | ce_chs 6.7787,3.7029,2.4898 | ang_mse 2.503659 dist_mse 0.222301 | accs 0.016,0.047,0.250 | w=[0.05097575 0.07208288 0.8769414 ] | lr=7.833e-05 | elapsed=300.7s\n",
      "step  18000/50000 | tot_loss 3.3042 | ce_chs 5.9594,3.9068,2.1458 | ang_mse 1.482563 dist_mse 0.264036 | accs 0.031,0.078,0.312 | w=[0.04527735 0.06387345 0.8908492 ] | lr=7.581e-05 | elapsed=319.9s\n",
      "step  19000/50000 | tot_loss 3.8362 | ce_chs 5.7236,3.8294,2.2993 | ang_mse 2.375911 dist_mse 0.247552 | accs 0.031,0.031,0.328 | w=[0.04033802 0.05686039 0.9028015 ] | lr=7.321e-05 | elapsed=341.7s\n",
      "step  20000/50000 | tot_loss 3.8723 | ce_chs 5.8820,3.3871,2.3915 | ang_mse 2.444605 dist_mse 0.164655 | accs 0.062,0.094,0.344 | w=[0.03600587 0.05071038 0.91328377] | lr=7.054e-05 | elapsed=361.4s\n",
      "step  21000/50000 | tot_loss 3.7405 | ce_chs 6.4528,3.5999,2.3032 | ang_mse 2.290432 dist_mse 0.199008 | accs 0.031,0.047,0.281 | w=[0.03223063 0.04537127 0.92239803] | lr=6.780e-05 | elapsed=383.8s\n",
      "step  22000/50000 | tot_loss 3.4185 | ce_chs 5.5754,3.7716,2.3814 | ang_mse 1.539046 dist_mse 0.236322 | accs 0.047,0.000,0.172 | w=[0.02900594 0.04079518 0.9301989 ] | lr=6.501e-05 | elapsed=401.3s\n",
      "step  23000/50000 | tot_loss 3.8893 | ce_chs 6.1072,3.3218,2.3606 | ang_mse 2.634510 dist_mse 0.156164 | accs 0.000,0.078,0.219 | w=[0.02616221 0.03677186 0.93706584] | lr=6.218e-05 | elapsed=419.2s\n",
      "step  24000/50000 | tot_loss 3.5460 | ce_chs 6.3213,3.8079,2.3836 | ang_mse 1.795429 dist_mse 0.248298 | accs 0.016,0.031,0.250 | w=[0.02367369 0.03327918 0.94304717] | lr=5.932e-05 | elapsed=435.8s\n",
      "step  25000/50000 | tot_loss 3.3177 | ce_chs 5.7241,3.5372,2.2133 | ang_mse 1.785477 dist_mse 0.192144 | accs 0.047,0.094,0.375 | w=[0.02152228 0.03023231 0.94824547] | lr=5.644e-05 | elapsed=454.4s\n",
      "step  26000/50000 | tot_loss 4.1003 | ce_chs 7.4633,3.6142,2.3844 | ang_mse 2.959834 dist_mse 0.204679 | accs 0.000,0.047,0.312 | w=[0.01962985 0.02756607 0.9528041 ] | lr=5.355e-05 | elapsed=470.4s\n",
      "step  27000/50000 | tot_loss 3.4110 | ce_chs 6.1417,4.2792,2.3361 | ang_mse 1.577581 dist_mse 0.337124 | accs 0.016,0.031,0.219 | w=[0.01799267 0.02525915 0.9567481 ] | lr=5.068e-05 | elapsed=489.0s\n",
      "step  28000/50000 | tot_loss 4.0750 | ce_chs 6.8923,3.4825,2.3102 | ang_mse 3.146871 dist_mse 0.176527 | accs 0.031,0.078,0.312 | w=[0.01656013 0.02324396 0.96019596] | lr=4.782e-05 | elapsed=505.2s\n",
      "step  29000/50000 | tot_loss 3.9507 | ce_chs 6.4958,3.6365,2.3158 | ang_mse 2.875617 dist_mse 0.209642 | accs 0.016,0.016,0.297 | w=[0.01529551 0.02146418 0.96324027] | lr=4.498e-05 | elapsed=520.9s\n",
      "step  30000/50000 | tot_loss 3.0612 | ce_chs 5.2974,3.8364,2.0802 | ang_mse 1.547470 dist_mse 0.253223 | accs 0.016,0.094,0.328 | w=[0.0141942  0.01991951 0.96588624] | lr=4.219e-05 | elapsed=536.9s\n",
      "step  31000/50000 | tot_loss 3.4451 | ce_chs 5.9104,3.5574,2.2850 | ang_mse 1.986781 dist_mse 0.190286 | accs 0.016,0.047,0.344 | w=[0.01322756 0.01856187 0.9682105 ] | lr=3.946e-05 | elapsed=552.2s\n",
      "step  32000/50000 | tot_loss 3.8627 | ce_chs 6.3222,3.3878,2.3936 | ang_mse 2.640894 dist_mse 0.165509 | accs 0.062,0.031,0.375 | w=[0.01237779 0.01737462 0.97024757] | lr=3.678e-05 | elapsed=567.1s\n",
      "step  33000/50000 | tot_loss 3.1741 | ce_chs 5.5142,3.6102,2.1879 | ang_mse 1.636060 dist_mse 0.212379 | accs 0.062,0.000,0.266 | w=[0.01163672 0.01632637 0.97203696] | lr=3.418e-05 | elapsed=581.9s\n",
      "step  34000/50000 | tot_loss 3.5242 | ce_chs 5.9854,3.6930,2.1811 | ang_mse 2.333130 dist_mse 0.222898 | accs 0.031,0.000,0.297 | w=[0.01098445 0.01540991 0.9736057 ] | lr=3.167e-05 | elapsed=598.1s\n",
      "step  35000/50000 | tot_loss 3.8438 | ce_chs 5.9162,3.9893,2.4074 | ang_mse 2.469115 dist_mse 0.284471 | accs 0.047,0.031,0.219 | w=[0.0104052  0.01459679 0.974998  ] | lr=2.925e-05 | elapsed=613.3s\n",
      "step  36000/50000 | tot_loss 3.4389 | ce_chs 6.0556,3.4658,2.4302 | ang_mse 1.746083 dist_mse 0.170673 | accs 0.047,0.031,0.312 | w=[0.00989711 0.01388689 0.9762161 ] | lr=2.694e-05 | elapsed=628.8s\n",
      "step  37000/50000 | tot_loss 3.7277 | ce_chs 5.6464,3.4871,2.5140 | ang_mse 2.159721 dist_mse 0.182760 | accs 0.031,0.047,0.188 | w=[0.00945049 0.01325845 0.97729105] | lr=2.474e-05 | elapsed=644.4s\n",
      "step  38000/50000 | tot_loss 3.3070 | ce_chs 6.3766,3.6931,2.1838 | ang_mse 1.910056 dist_mse 0.221923 | accs 0.016,0.125,0.312 | w=[0.00905814 0.01270703 0.9782348 ] | lr=2.267e-05 | elapsed=661.1s\n",
      "step  39000/50000 | tot_loss 3.4619 | ce_chs 5.7406,3.5074,2.2567 | ang_mse 2.128417 dist_mse 0.190800 | accs 0.016,0.031,0.312 | w=[0.00870961 0.01221711 0.9790733 ] | lr=2.073e-05 | elapsed=676.5s\n",
      "step  40000/50000 | tot_loss 3.5295 | ce_chs 6.1002,3.9335,2.5649 | ang_mse 1.575704 dist_mse 0.261766 | accs 0.047,0.016,0.203 | w=[0.00840327 0.0117879  0.97980887] | lr=1.893e-05 | elapsed=694.1s\n",
      "step  41000/50000 | tot_loss 3.2835 | ce_chs 6.2466,3.9842,2.2575 | ang_mse 1.666496 dist_mse 0.281316 | accs 0.016,0.000,0.328 | w=[0.00813173 0.01140932 0.980459  ] | lr=1.728e-05 | elapsed=711.8s\n",
      "step  42000/50000 | tot_loss 3.6439 | ce_chs 6.3232,3.7744,2.2226 | ang_mse 2.502236 dist_mse 0.241297 | accs 0.016,0.000,0.188 | w=[0.00789143 0.01107176 0.9810368 ] | lr=1.579e-05 | elapsed=728.3s\n",
      "step  43000/50000 | tot_loss 3.1066 | ce_chs 5.8181,3.2016,2.0028 | ang_mse 2.000039 dist_mse 0.123243 | accs 0.016,0.094,0.359 | w=[0.00767665 0.01077014 0.9815532 ] | lr=1.446e-05 | elapsed=744.3s\n",
      "step  44000/50000 | tot_loss 3.4344 | ce_chs 6.6406,3.4139,2.1155 | ang_mse 2.383782 dist_mse 0.158904 | accs 0.031,0.125,0.375 | w=[0.00748193 0.01049748 0.98202056] | lr=1.329e-05 | elapsed=761.3s\n",
      "step  45000/50000 | tot_loss 3.6336 | ce_chs 5.8857,3.8134,2.4102 | ang_mse 2.112218 dist_mse 0.255128 | accs 0.016,0.016,0.312 | w=[0.00730754 0.01025347 0.9824389 ] | lr=1.229e-05 | elapsed=778.2s\n",
      "step  46000/50000 | tot_loss 3.8351 | ce_chs 6.0641,3.6906,2.7562 | ang_mse 1.876168 dist_mse 0.215460 | accs 0.016,0.078,0.172 | w=[0.0071503  0.01003286 0.9828168 ] | lr=1.147e-05 | elapsed=798.0s\n",
      "step  47000/50000 | tot_loss 3.7859 | ce_chs 6.7637,3.6650,2.3321 | ang_mse 2.611335 dist_mse 0.207901 | accs 0.000,0.078,0.328 | w=[0.00700514 0.00982977 0.983165  ] | lr=1.083e-05 | elapsed=815.5s\n",
      "step  48000/50000 | tot_loss 3.0184 | ce_chs 5.5738,3.7987,2.1147 | ang_mse 1.486179 dist_mse 0.241191 | accs 0.031,0.109,0.312 | w=[0.00686863 0.00963934 0.9834921 ] | lr=1.037e-05 | elapsed=834.1s\n",
      "step  49000/50000 | tot_loss 3.7304 | ce_chs 6.2046,4.0480,2.2399 | ang_mse 2.609019 dist_mse 0.284328 | accs 0.016,0.078,0.391 | w=[0.00673961 0.00945835 0.98380196] | lr=1.009e-05 | elapsed=853.8s\n",
      "Training finished. Total time: 878.6538178920746\n",
      "\n",
      "Demo window (t0,t1,t2) tokens: ['r', 'a', 'y']\n",
      "Demo prediction (channel topk & regression outputs):\n",
      "{'angle_pred_rad': 1.4208942651748657, 'dist_pred': 1.1017944812774658, 'ch1_top': [('e', 0.16322915256023407), ('b', 0.12292492389678955), ('m', 0.11523734778165817), ('t', 0.10297414660453796), ('EOT', 0.099853515625), ('a', 0.09552180767059326)], 'ch2_top': [('w', 0.0889802798628807), ('e', 0.08796964585781097), ('u', 0.08796375244855881), ('m', 0.08360619097948074), ('n', 0.08019104599952698), ('b', 0.07364106923341751)], 'ch3_top': [('l', 0.2624070942401886), ('a', 0.18655072152614594), ('EOT', 0.1586999148130417), ('n', 0.07817094027996063), ('d', 0.0689445436000824), ('s', 0.04864443093538284)], 'channel_weights': array([0.00661682, 0.00928575, 0.9840974 ], dtype=float32)}\n",
      "Saved checkpoint: three_channel_small_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "# Colab-ready: three-channel small model training on EurekaLabsAI/mlp data\n",
    "# Paste into a single Colab cell and run.\n",
    "# NOTE: Default steps = 50000. For quick debug, set MAX_STEPS = 2000.\n",
    "\n",
    "import os, math, random, requests, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG (change these for quick debugging)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# vocab: a-z + EOT\n",
    "VOCAB = [chr(ord('a') + i) for i in range(26)] + ['EOT']\n",
    "V = len(VOCAB)\n",
    "EOT_IDX = 26\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Data URLs (raw files)\n",
    "RAW_BASE = \"https://raw.githubusercontent.com/EurekaLabsAI/mlp/master/data\"\n",
    "URLS = {\n",
    "    \"train\": f\"{RAW_BASE}/train.txt\",\n",
    "    \"val\":   f\"{RAW_BASE}/val.txt\",\n",
    "    \"test\":  f\"{RAW_BASE}/test.txt\",\n",
    "}\n",
    "\n",
    "DATA_DIR = Path(\"data_repo\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# coordinates: we sample in [-100,100] then normalize to [-1,1]\n",
    "EMBED_SPACE = 100.0\n",
    "\n",
    "# training hyperparams\n",
    "BATCH_SIZE = 64\n",
    "MAX_STEPS = 50000  # default as you requested; reduce for quick tests (e.g., 2000)\n",
    "PRINT_EVERY = 1000\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 1000\n",
    "\n",
    "# -------------------------\n",
    "# download function\n",
    "def download_if_missing():\n",
    "    for name, url in URLS.items():\n",
    "        dest = DATA_DIR / f\"{name}.txt\"\n",
    "        if dest.exists():\n",
    "            print(f\"{dest} exists, skipping download.\")\n",
    "            continue\n",
    "        print(f\"Downloading {name} from {url} ...\")\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(f\"Failed to download {url}: HTTP {r.status_code}\")\n",
    "        dest.write_text(r.text, encoding=\"utf-8\")\n",
    "        print(f\"Saved {dest} ({len(r.text)} bytes)\")\n",
    "\n",
    "# -------------------------\n",
    "# tokenization\n",
    "def text_to_token_ids(text):\n",
    "    ids = []\n",
    "    for ch in text.lower():\n",
    "        if 'a' <= ch <= 'z':\n",
    "            idx = ord(ch) - ord('a')\n",
    "            ids.append(idx)\n",
    "        else:\n",
    "            ids.append(EOT_IDX)\n",
    "    return ids\n",
    "\n",
    "# -------------------------\n",
    "# Dataset -> sliding windows (t0,t1,t2)->t3\n",
    "class SlidingTextDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        txt = Path(file_path).read_text(encoding='utf-8')\n",
    "        ids = text_to_token_ids(txt)\n",
    "        self.windows = []\n",
    "        for i in range(len(ids) - 3):\n",
    "            self.windows.append((np.int64(ids[i]), np.int64(ids[i+1]), np.int64(ids[i+2]), np.int64(ids[i+3])))\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    def __getitem__(self, idx):\n",
    "        t0,t1,t2,t3 = self.windows[idx]\n",
    "        return {\"t0\": t0, \"t1\": t1, \"t2\": t2, \"t3\": t3}\n",
    "\n",
    "# -------------------------\n",
    "# fixed coordinates generator (seeded)\n",
    "def make_fixed_coords(seed=SEED, vocab_size=V, embed_space=EMBED_SPACE):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    raw = rng.uniform(low=-embed_space, high=embed_space, size=(vocab_size, 2)).astype(np.float32)\n",
    "    coords = raw / embed_space   # normalized to [-1,1] floats\n",
    "    return torch.from_numpy(coords)  # (V,2)\n",
    "\n",
    "# -------------------------\n",
    "# small 3-channel model (well under 100k params)\n",
    "class ThreeChannelPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, coords_fixed_tensor, emb_dim=16, hidden=64):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.coords_fixed = coords_fixed_tensor.to(DEVICE)  # not learnable (fixed)\n",
    "        # Channel3 token embedding (small)\n",
    "        self.tok_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        # Channel1 (angle): input 6 floats -> small MLP -> scalar\n",
    "        self.ch1_mlp = nn.Sequential(\n",
    "            nn.Linear(6, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden//4, 1)\n",
    "        )\n",
    "        # Channel2 (distance): same shape\n",
    "        self.ch2_mlp = nn.Sequential(\n",
    "            nn.Linear(6, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden//4, 1)\n",
    "        )\n",
    "        # Channel3: sequence token classifier from concatenated token embeddings\n",
    "        self.ch3_mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 3, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, vocab_size)\n",
    "        )\n",
    "        # trainable raw weights for combining CE channel losses (softmaxed)\n",
    "        self.raw_channel_weights = nn.Parameter(torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, t0_ids, t1_ids, t2_ids):\n",
    "        # gather coords for geometry channels\n",
    "        batch = t0_ids.shape[0]\n",
    "        c0 = self.coords_fixed[t0_ids].to(DEVICE)  # (batch,2)\n",
    "        c1 = self.coords_fixed[t1_ids].to(DEVICE)\n",
    "        c2 = self.coords_fixed[t2_ids].to(DEVICE)\n",
    "        inp = torch.cat([c0, c1, c2], dim=-1)  # (batch,6)\n",
    "\n",
    "        # CH1 angle regression (tanh -> [-pi, pi])\n",
    "        ang_raw = self.ch1_mlp(inp).squeeze(-1)\n",
    "        ang_pred = torch.tanh(ang_raw) * math.pi  # (batch,)\n",
    "\n",
    "        # CH2 distance regression (tanh -> [0, max_possible_dist])\n",
    "        dist_raw = self.ch2_mlp(inp).squeeze(-1)\n",
    "        max_possible_dist = math.sqrt(8.0)  # points in [-1,1]^2\n",
    "        dist_pred = (torch.tanh(dist_raw) + 1.0) * 0.5 * max_possible_dist  # (batch,)\n",
    "\n",
    "        # CH1 logits: compare predicted direction to directions from c2 to all token coords\n",
    "        dir_pred = torch.stack([torch.cos(ang_pred), torch.sin(ang_pred)], dim=-1)  # (batch,2)\n",
    "        coords_exp = self.coords_fixed.unsqueeze(0).expand(batch, -1, -1)  # (batch,V,2)\n",
    "        c2_exp = c2.unsqueeze(1).expand(-1, self.vocab_size, -1)         # (batch,V,2)\n",
    "        dir_to_tokens = coords_exp - c2_exp\n",
    "        norm = torch.norm(dir_to_tokens, dim=-1, keepdim=True).clamp(min=1e-6)\n",
    "        dir_to_tokens_unit = dir_to_tokens / norm\n",
    "        ch1_logits = torch.sum(dir_pred.unsqueeze(1) * dir_to_tokens_unit, dim=-1)  # cos similarity (batch,V)\n",
    "        ch1_logits = ch1_logits * 8.0  # temperature sharpening\n",
    "\n",
    "        # CH2 logits: closeness of predicted distance to actual distances\n",
    "        dists_to_tokens = torch.norm(coords_exp - c2_exp, dim=-1)  # (batch,V)\n",
    "        ch2_logits = - (dists_to_tokens - dist_pred.unsqueeze(1)) ** 2\n",
    "        ch2_logits = ch2_logits * 5.0\n",
    "\n",
    "        # CH3 logits: classic token classifier\n",
    "        t0_emb = self.tok_embed(t0_ids)\n",
    "        t1_emb = self.tok_embed(t1_ids)\n",
    "        t2_emb = self.tok_embed(t2_ids)\n",
    "        ch3_in = torch.cat([t0_emb, t1_emb, t2_emb], dim=-1)  # (batch, emb_dim*3)\n",
    "        ch3_logits = self.ch3_mlp(ch3_in)  # (batch, V)\n",
    "\n",
    "        w = F.softmax(self.raw_channel_weights, dim=0)  # normalized trainable weights (3,)\n",
    "        return {\n",
    "            \"ang_pred\": ang_pred,\n",
    "            \"dist_pred\": dist_pred,\n",
    "            \"ch1_logits\": ch1_logits,\n",
    "            \"ch2_logits\": ch2_logits,\n",
    "            \"ch3_logits\": ch3_logits,\n",
    "            \"channel_weights\": w\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# helper functions\n",
    "def angle_between(a, b):\n",
    "    vec = b - a\n",
    "    return torch.atan2(vec[...,1], vec[...,0])\n",
    "\n",
    "def distance_between(a, b):\n",
    "    return torch.norm(b - a, dim=-1)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "# -------------------------\n",
    "# Prepare data\n",
    "print(\"Downloading data if needed...\")\n",
    "download_if_missing()\n",
    "train_ds = SlidingTextDataset(DATA_DIR / \"train.txt\")\n",
    "val_ds   = SlidingTextDataset(DATA_DIR / \"val.txt\")\n",
    "test_ds  = SlidingTextDataset(DATA_DIR / \"test.txt\")\n",
    "print(\"Dataset sizes -> train:\", len(train_ds), \"val:\", len(val_ds), \"test:\", len(test_ds))\n",
    "\n",
    "# use deterministic generator for DataLoader shuffle\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0, generator=g)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "# -------------------------\n",
    "# Make coords and model\n",
    "coords = make_fixed_coords(seed=SEED)  # (V,2), floats in [-1,1]\n",
    "print(\"Token coords (first 8):\")\n",
    "for i in range(min(8, V)):\n",
    "    print(f\"  {VOCAB[i]:>3s} idx={i:2d} -> {coords[i].numpy()}\")\n",
    "\n",
    "# plot coords (optional): requires matplotlib\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    xy = coords.numpy()\n",
    "    ax.scatter(xy[:,0], xy[:,1], c='tab:blue')\n",
    "    for i, lab in enumerate(VOCAB):\n",
    "        ax.text(xy[i,0], xy[i,1], lab, fontsize=9)\n",
    "    ax.set_title(\"Token coordinates (normalized to [-1,1])\")\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Plot skipped (matplotlib not available?) -\", e)\n",
    "\n",
    "# instantiate model with small dims (emb_dim=16, hidden=64)\n",
    "model = ThreeChannelPredictor(V, coords, emb_dim=16, hidden=64).to(DEVICE)\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nModel instanced. Total params: {total_params:,}, Trainable: {trainable_params:,}\")\n",
    "assert trainable_params <= 100_000, f\"Trainable params must be <=100k, but got {trainable_params}\"\n",
    "\n",
    "# DEBUG prints: show token embedding (first rows), raw_channel_weights initial\n",
    "print(\"\\nToken embedding (first 6 rows):\")\n",
    "print(model.tok_embed.weight.data[:6])\n",
    "\n",
    "print(\"\\nraw_channel_weights (before softmax):\", model.raw_channel_weights.data)\n",
    "print(\"channel weights (softmax):\", F.softmax(model.raw_channel_weights, dim=0).detach().cpu().numpy())\n",
    "\n",
    "# Print model summary (architecture)\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Print per-parameter table (fixed formatting)\n",
    "print(\"\\nParameters (name, shape, #params, trainable):\")\n",
    "for name, p in model.named_parameters():\n",
    "    shape_str = str(tuple(p.shape))\n",
    "    print(f\" - {name:40s} | shape {shape_str:20s} | # {p.numel():6d} | trainable={p.requires_grad}\")\n",
    "\n",
    "# -------------------------\n",
    "# Losses, optimizer, scheduler\n",
    "mse_loss = nn.MSELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return float(step) / max(1.0, WARMUP_STEPS)\n",
    "    progress = float(step - WARMUP_STEPS) / max(1, MAX_STEPS - WARMUP_STEPS)\n",
    "    return 0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# -------------------------\n",
    "# Training loop\n",
    "print_every = PRINT_EVERY\n",
    "step = 0\n",
    "it = iter(train_loader)\n",
    "model.train()\n",
    "t0_time = time.time()\n",
    "print(\"\\nStarting training loop (steps = {})\".format(MAX_STEPS))\n",
    "while step < MAX_STEPS:\n",
    "    try:\n",
    "        batch = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(train_loader)\n",
    "        batch = next(it)\n",
    "\n",
    "    t0 = torch.tensor(batch[\"t0\"], dtype=torch.long, device=DEVICE)\n",
    "    t1 = torch.tensor(batch[\"t1\"], dtype=torch.long, device=DEVICE)\n",
    "    t2 = torch.tensor(batch[\"t2\"], dtype=torch.long, device=DEVICE)\n",
    "    t3 = torch.tensor(batch[\"t3\"], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    out = model(t0, t1, t2)\n",
    "\n",
    "    # compute regression targets from fixed coords\n",
    "    c2 = coords[t2].to(DEVICE)\n",
    "    c3 = coords[t3].to(DEVICE)\n",
    "    ang_target = angle_between(c2, c3)\n",
    "    dist_target = distance_between(c2, c3)\n",
    "\n",
    "    loss_ang = mse_loss(out[\"ang_pred\"], ang_target)\n",
    "    loss_dist = mse_loss(out[\"dist_pred\"], dist_target)\n",
    "\n",
    "    loss_ce_ch1 = ce_loss(out[\"ch1_logits\"], t3)\n",
    "    loss_ce_ch2 = ce_loss(out[\"ch2_logits\"], t3)\n",
    "    loss_ce_ch3 = ce_loss(out[\"ch3_logits\"], t3)\n",
    "\n",
    "    w = out[\"channel_weights\"]\n",
    "    loss_ce = w[0]*loss_ce_ch1 + w[1]*loss_ce_ch2 + w[2]*loss_ce_ch3\n",
    "\n",
    "    total_loss = loss_ce + 0.5 * loss_ang + 0.5 * loss_dist\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if (step % print_every) == 0:\n",
    "        # compute small diagnostics (accuracy per channel on this batch)\n",
    "        with torch.no_grad():\n",
    "            acc1 = (out[\"ch1_logits\"].argmax(dim=-1) == t3).float().mean().item()\n",
    "            acc2 = (out[\"ch2_logits\"].argmax(dim=-1) == t3).float().mean().item()\n",
    "            acc3 = (out[\"ch3_logits\"].argmax(dim=-1) == t3).float().mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - t0_time\n",
    "        print(f\"step {step:6d}/{MAX_STEPS} | tot_loss {total_loss.item():.4f} | ce_chs {loss_ce_ch1.item():.4f},{loss_ce_ch2.item():.4f},{loss_ce_ch3.item():.4f} | \"\n",
    "              f\"ang_mse {loss_ang.item():.6f} dist_mse {loss_dist.item():.6f} | accs {acc1:.3f},{acc2:.3f},{acc3:.3f} | w={w.cpu().detach().numpy()} | lr={lr_now:.3e} | elapsed={elapsed:.1f}s\")\n",
    "    step += 1\n",
    "\n",
    "print(\"Training finished. Total time:\", time.time() - t0_time)\n",
    "\n",
    "# -------------------------\n",
    "# Small evaluation helper and example\n",
    "def predict_next(model, t0_id, t1_id, t2_id, topk=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t0t = torch.tensor([t0_id], dtype=torch.long, device=DEVICE)\n",
    "        t1t = torch.tensor([t1_id], dtype=torch.long, device=DEVICE)\n",
    "        t2t = torch.tensor([t2_id], dtype=torch.long, device=DEVICE)\n",
    "        out = model(t0t, t1t, t2t)\n",
    "        p1 = F.softmax(out[\"ch1_logits\"], dim=-1).cpu().numpy()[0]\n",
    "        p2 = F.softmax(out[\"ch2_logits\"], dim=-1).cpu().numpy()[0]\n",
    "        p3 = F.softmax(out[\"ch3_logits\"], dim=-1).cpu().numpy()[0]\n",
    "        def topk_str(p):\n",
    "            idx = np.argsort(-p)[:topk]\n",
    "            return [(VOCAB[i], float(p[i])) for i in idx]\n",
    "        return {\n",
    "            \"angle_pred_rad\": float(out[\"ang_pred\"].cpu().numpy()[0]),\n",
    "            \"dist_pred\": float(out[\"dist_pred\"].cpu().numpy()[0]),\n",
    "            \"ch1_top\": topk_str(p1),\n",
    "            \"ch2_top\": topk_str(p2),\n",
    "            \"ch3_top\": topk_str(p3),\n",
    "            \"channel_weights\": out[\"channel_weights\"].cpu().numpy()\n",
    "        }\n",
    "\n",
    "# demo prediction on first window\n",
    "if len(train_ds) > 0:\n",
    "    demo_window = train_ds.windows[0]\n",
    "    print(\"\\nDemo window (t0,t1,t2) tokens:\", [VOCAB[i] for i in demo_window[:3]])\n",
    "    demo = predict_next(model, demo_window[0], demo_window[1], demo_window[2], topk=6)\n",
    "    print(\"Demo prediction (channel topk & regression outputs):\")\n",
    "    print(demo)\n",
    "\n",
    "# save model checkpoint (optional)\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"coords\": coords,\n",
    "    \"vocab\": VOCAB\n",
    "}, \"three_channel_small_checkpoint.pt\")\n",
    "print(\"Saved checkpoint: three_channel_small_checkpoint.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75d0887c-2ab0-42ae-97ac-3da609a12517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e66322b6-cd29-4f84-98d1-967f2b67c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: rayleyEOT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ---------- utility: top-k / top-p filtering for a probability vector ----------\n",
    "def apply_top_k_top_p(probs, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    probs: 1D torch tensor (V,) of non-negative probabilities (sums to 1)\n",
    "    returns: filtered probs (same shape), re-normalized to sum=1\n",
    "    \"\"\"\n",
    "    V = probs.size(0)\n",
    "    p = probs.clone()\n",
    "\n",
    "    # TOP-K\n",
    "    if top_k is not None and top_k > 0 and top_k < V:\n",
    "        # keep only top_k probs\n",
    "        vals, idx = torch.topk(p, top_k)\n",
    "        mask = torch.full_like(p, 0.0)\n",
    "        mask[idx] = 1.0\n",
    "        p = p * mask\n",
    "\n",
    "    # TOP-P (nucleus)\n",
    "    if top_p is not None and 0.0 < top_p < 1.0:\n",
    "        sorted_p, sorted_idx = torch.sort(p, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_p, dim=0)\n",
    "        # keep tokens where cumulative prob <= top_p (inclusive of first that exceeds)\n",
    "        cutoff = (cumsum <= top_p)\n",
    "        # ensure at least one token retained\n",
    "        if not cutoff.any():\n",
    "            cutoff[0] = True\n",
    "        keep_count = cutoff.sum().item()\n",
    "        # create mask\n",
    "        topk_idx = sorted_idx[:keep_count]\n",
    "        mask = torch.zeros_like(p)\n",
    "        mask[topk_idx] = 1.0\n",
    "        p = p * mask\n",
    "\n",
    "    # if everything zeroed (rare), fallback to original probs\n",
    "    if p.sum() <= 0.0:\n",
    "        p = probs.clone()\n",
    "\n",
    "    # renormalize\n",
    "    p = p / p.sum()\n",
    "    return p\n",
    "\n",
    "# ---------- function: sample a single next token distribution and sample ----------\n",
    "def sample_next_token(model, t0_id, t1_id, t2_id, temperature=1.0, top_k=None, top_p=None, deterministic=False):\n",
    "    \"\"\"\n",
    "    model: trained ThreeChannelPredictor (in eval mode)\n",
    "    t*_id: scalar ints (token indices) or 0-d torch tensors on CPU\n",
    "    Returns: sampled_token_idx (int), diagnostics dict\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # prepare tensors on same device as model\n",
    "        dev = next(model.parameters()).device\n",
    "        t0 = torch.tensor([t0_id], dtype=torch.long, device=dev)\n",
    "        t1 = torch.tensor([t1_id], dtype=torch.long, device=dev)\n",
    "        t2 = torch.tensor([t2_id], dtype=torch.long, device=dev)\n",
    "\n",
    "        out = model(t0, t1, t2)\n",
    "        w = out[\"channel_weights\"].to(dev)        # shape (3,)\n",
    "\n",
    "        # get per-channel logits and convert to probabilities (scale by temperature)\n",
    "        # Note: temperature applied to logits is equivalent to dividing logits by temperature.\n",
    "        eps = 1e-12\n",
    "        temp = max(1e-8, float(temperature))\n",
    "\n",
    "        p1 = F.softmax(out[\"ch1_logits\"].squeeze(0) / temp, dim=-1)  # angle-channel probs\n",
    "        p2 = F.softmax(out[\"ch2_logits\"].squeeze(0) / temp, dim=-1)  # distance-channel probs\n",
    "        p3 = F.softmax(out[\"ch3_logits\"].squeeze(0) / temp, dim=-1)  # token-id-channel probs\n",
    "\n",
    "        # Weighted mixture of probabilities (using learned channel weights)\n",
    "        combined = w[0]*p1 + w[1]*p2 + w[2]*p3\n",
    "        combined = combined / (combined.sum() + eps)\n",
    "\n",
    "        # apply top-k / top-p on combined distribution\n",
    "        filtered = apply_top_k_top_p(combined, top_k=top_k, top_p=top_p)\n",
    "\n",
    "        # sampling\n",
    "        if deterministic or temperature == 0.0:\n",
    "            next_idx = int(filtered.argmax().item())\n",
    "        else:\n",
    "            # torch.multinomial expects float nonnegative sum-to-1 tensor\n",
    "            next_idx = int(torch.multinomial(filtered, num_samples=1).item())\n",
    "\n",
    "        # return also per-channel topk for debugging\n",
    "        def topk_list(p, k=5):\n",
    "            vals, idx = torch.topk(p, min(k, p.size(0)))\n",
    "            return [(int(i.item()), float(v.item())) for v,i in zip(vals, idx)]\n",
    "\n",
    "        diagnostics = {\n",
    "            \"combined_probs\": combined.cpu().numpy(),\n",
    "            \"filtered_probs\": filtered.cpu().numpy(),\n",
    "            \"ch1_top5\": topk_list(p1.cpu(), 5),\n",
    "            \"ch2_top5\": topk_list(p2.cpu(), 5),\n",
    "            \"ch3_top5\": topk_list(p3.cpu(), 5),\n",
    "            \"channel_weights\": w.cpu().numpy(),\n",
    "            \"ang_pred\": float(out[\"ang_pred\"].cpu().numpy()[0]),\n",
    "            \"dist_pred\": float(out[\"dist_pred\"].cpu().numpy()[0])\n",
    "        }\n",
    "        return next_idx, diagnostics\n",
    "\n",
    "# ---------- function: autoregressive sampling of a sequence ----------\n",
    "def sample_sequence(model, start_tokens, length=50, temperature=1.0, top_k=None, top_p=None, stop_on_eot=True, deterministic=False, seed=None):\n",
    "    \"\"\"\n",
    "    start_tokens: list of token indices (ints) or string (e.g., 'abc') -> if string, converted to indices a..z and others -> EOT\n",
    "    length: total number of tokens to generate (not counting the provided start tokens)\n",
    "    Returns: generated_indices list (including start), diagnostics_per_step list\n",
    "    \"\"\"\n",
    "    # helper to map char->idx if start_tokens is string\n",
    "    def char_to_idx(ch):\n",
    "        ch = ch.lower()\n",
    "        if 'a' <= ch <= 'z':\n",
    "            return ord(ch) - ord('a')\n",
    "        else:\n",
    "            return 26  # EOT\n",
    "\n",
    "    # prepare initial context (need at least 3 tokens). If shorter, pad with EOT on the left.\n",
    "    if isinstance(start_tokens, str):\n",
    "        ctx = [char_to_idx(c) for c in start_tokens]\n",
    "    else:\n",
    "        ctx = list(start_tokens)\n",
    "\n",
    "    # left-pad with EOTs if ctx shorter than 3\n",
    "    while len(ctx) < 3:\n",
    "        ctx.insert(0, 26)\n",
    "\n",
    "    # optionally set seed for reproducibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    generated = list(ctx)   # full token sequence including initial\n",
    "    diagnostics = []\n",
    "\n",
    "    model.eval()\n",
    "    for step in range(length):\n",
    "        t0, t1, t2 = generated[-3], generated[-2], generated[-1]\n",
    "        nxt, diag = sample_next_token(model, t0, t1, t2, temperature=temperature, top_k=top_k, top_p=top_p, deterministic=deterministic)\n",
    "        generated.append(nxt)\n",
    "        diagnostics.append(diag)\n",
    "        # stop early on EOT if requested\n",
    "        if stop_on_eot and nxt == 26:\n",
    "            break\n",
    "\n",
    "    return generated, diagnostics\n",
    "\n",
    "# ---------- Example usage (after training) ----------\n",
    "# Suppose `model` and `VOCAB` are available in the notebook\n",
    "# Example: start from the first window in your training set:\n",
    "demo_window = train_ds.windows[0]   # (t0,t1,t2,t3)\n",
    "start_ctx = demo_window[:3]\n",
    "seq, diags = sample_sequence(model, start_ctx, length=30, temperature=1.0, top_k=5, seed=42)\n",
    "#Convert to tokens:\n",
    "token_seq = [VOCAB[i] for i in seq]\n",
    "print(\"Generated:\", ''.join(token_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3860765-8665-42a8-82e5-5ff842e04511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 = a ... 26 = \\n\n",
      "a\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "0 a\n",
      "1 b\n",
      "2 c\n",
      "3 d\n",
      "4 e\n",
      "5 f\n",
      "6 g\n",
      "7 h\n",
      "8 i\n",
      "9 j\n",
      "10 k\n",
      "11 l\n",
      "12 m\n",
      "13 n\n",
      "14 o\n",
      "15 p\n",
      "16 q\n",
      "17 r\n",
      "18 s\n",
      "19 t\n",
      "20 u\n",
      "21 v\n",
      "22 w\n",
      "23 x\n",
      "24 y\n",
      "25 z\n",
      "test >>  l\n",
      "test >>  y\n",
      "test >>  n\n",
      "prompt_add >>  [11, 24, 13]\n",
      "[17, 0, 24]\n",
      "Starting context (from train_ds first window): ['l', 'y', 'n']\n",
      "\n",
      "--- Generated (100 tokens + context) ---\n",
      "\n",
      "lynn\n",
      "ameri\n",
      "marion\n",
      "myranilen\n",
      "amera\n",
      "marie\n",
      "jiela\n",
      "kerena\n",
      "kyn\n",
      "mai\n",
      "jamar\n",
      "mera\n",
      "dan\n",
      "mareen\n",
      "jere\n",
      "korrin\n",
      "kairie\n",
      "r\n",
      "\n",
      "Generated indices (first 50 shown): [11, 24, 13, 13, 26, 0, 12, 4, 17, 8, 26, 12, 0, 17, 8, 14, 13, 26, 12, 24, 17, 0, 13, 8, 11, 4, 13, 26, 0, 12, 4, 17, 0, 26, 12, 0, 17, 8, 4, 26, 9, 8, 4, 11, 0, 26, 10, 4, 17, 4]\n",
      "Last-step channel weights: [0.00661682 0.00928575 0.9840974 ]\n",
      "Last-step top3 per-channel (ch1/ch2/ch3):\n",
      " CH1 top3 indices+probs: [(25, 0.28109219670295715), (17, 0.27800145745277405), (6, 0.15120220184326172)]\n",
      " CH2 top3 indices+probs: [(6, 0.08045969158411026), (22, 0.07726635783910751), (23, 0.07524733245372772)]\n",
      " CH3 top3 indices+probs: [(0, 0.1505446881055832), (10, 0.10014326870441437), (12, 0.08648804575204849)]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Sample 100 tokens and print, mapping EOT (idx 26) -> '\\n' ----------\n",
    "# Requires: sample_sequence(model, ...) and VOCAB and train_ds present in the notebook.\n",
    "\n",
    "# map displayed token -> show '\\n' for EOT index 26\n",
    "def display_token(idx):\n",
    "    if int(idx) == 26:\n",
    "        return '\\n'\n",
    "    else:\n",
    "        return VOCAB[int(idx)]\n",
    "\n",
    "print(\" 0 = a ... 26 = \\\\n\")\n",
    "print(display_token(0))\n",
    "print(display_token(26))\n",
    "print(40*\"-\")\n",
    "\n",
    "# lyn 11 24 13 \n",
    "for ijk in range(26):\n",
    "    print(ijk, display_token(ijk))\n",
    "############\n",
    "prompt_to_conv = \"lyn\"\n",
    "prompt_add = []\n",
    "for hgj in prompt_to_conv:\n",
    "    for mmj in range(26):\n",
    "        if display_token(mmj) == hgj:\n",
    "            print('test >> ', hgj)\n",
    "            prompt_add.append(mmj)\n",
    "            break\n",
    "\n",
    "print(\"prompt_add >> \", prompt_add)\n",
    "############\n",
    "\n",
    "# prepare a start context: try to use first training window if available, else fallback to 'the'\n",
    "if 'train_ds' in globals() and len(train_ds) > 0:\n",
    "    start_ctx = list(train_ds.windows[0][:3])  # (t0,t1,t2)\n",
    "    print(start_ctx)\n",
    "    #start_ctx = list([11, 24, 13]) # lyn\n",
    "    start_ctx = prompt_add\n",
    "    print(\"Starting context (from train_ds first window):\", [VOCAB[i] if i<26 else '\\\\n' for i in start_ctx])\n",
    "else:\n",
    "    # fallback prompt \"the\" -> indices for 't','h','e'\n",
    "    start_ctx = [ord('t')-ord('a'), ord('h')-ord('a'), ord('e')-ord('a')]\n",
    "    print(\"train_ds not found, using fallback start context:\", [VOCAB[i] for i in start_ctx])\n",
    "\n",
    "# Generate 100 tokens (not stopping on EOT), top_k sampling for safer results\n",
    "generated_indices, diagnostics = sample_sequence(\n",
    "    model,\n",
    "    start_tokens=start_ctx,\n",
    "    length=100,            # produce 100 new tokens\n",
    "    temperature=1.0,\n",
    "    top_k=5,\n",
    "    top_p=None,\n",
    "    stop_on_eot=False,     # DO NOT stop on EOT\n",
    "    deterministic=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Convert indices to displayable characters, mapping idx 26 -> newline\n",
    "display_chars = [display_token(i) for i in generated_indices]\n",
    "\n",
    "# Print as a continuous string (newlines will be rendered)\n",
    "print(\"\\n--- Generated (100 tokens + context) ---\\n\")\n",
    "print(''.join(display_chars))\n",
    "\n",
    "# Also show the raw indices and the per-step channel weights for the last step (as extra debug)\n",
    "print(\"\\nGenerated indices (first 50 shown):\", generated_indices[:50])\n",
    "if diagnostics:\n",
    "    last_diag = diagnostics[-1]\n",
    "    print(\"Last-step channel weights:\", last_diag.get(\"channel_weights\"))\n",
    "    print(\"Last-step top3 per-channel (ch1/ch2/ch3):\")\n",
    "    print(\" CH1 top3 indices+probs:\", last_diag[\"ch1_top5\"][:3])\n",
    "    print(\" CH2 top3 indices+probs:\", last_diag[\"ch2_top5\"][:3])\n",
    "    print(\" CH3 top3 indices+probs:\", last_diag[\"ch3_top5\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a19a1160-17f4-46bc-b98b-64f6d6743223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start context (tokens): ['l', 'y', 'n']\n",
      "\n",
      "Detailed per-step diagnostics (first 100 steps):\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "step chosen  idx         ch_weights comb_prob   ch1_ce   ch2_ce   ch3_ce   comb_ce ang_pred(rad) ang_tgt(rad)  ang_sqerr  dist_pred   dist_tgt dist_sqerr\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "   0    'n'   13 [0.007, 0.009, 0.984]    0.3189   10.015    8.415    1.127     1.143        0.255        0.000    0.06521     1.0686     0.0000   1.14196\n",
      "   1  '\\\\n'   26 [0.007, 0.009, 0.984]    0.6143    2.167    3.863    0.473     0.487        0.278        0.475    0.03879     1.1612     1.6640   0.25276\n",
      "   2    'a'    0 [0.007, 0.009, 0.984]    0.1028   11.701    2.676    2.265     2.275       -1.370        3.043   19.47684     1.3269     1.1354   0.03666\n",
      "   3    'm'   12 [0.007, 0.009, 0.984]    0.1056    2.235    4.529    2.240     2.248       -0.990       -1.113    0.01509     0.9849     0.3691   0.37924\n",
      "   4    'e'    4 [0.007, 0.009, 0.984]    0.1540    1.632    3.660    1.865     1.871       -0.692       -0.489    0.04153     0.7481     0.3285   0.17600\n",
      "   5    'r'   17 [0.007, 0.009, 0.984]    0.3373    3.501    2.756    1.073     1.087       -0.614        0.269    0.77866     0.7015     0.7561   0.00299\n",
      "   6    'i'    8 [0.007, 0.009, 0.984]    0.1659    8.992    2.662    1.784     1.796       -0.954       -2.736    3.17810     1.2108     1.4393   0.05220\n",
      "   7  '\\\\n'   26 [0.007, 0.009, 0.984]    0.1802    2.521    5.168    1.701     1.714        0.007        0.527    0.27067     0.7887     1.4704   0.46464\n",
      "   8    'm'   12 [0.007, 0.009, 0.984]    0.0749    8.793    3.268    2.581     2.592       -1.430       -2.919    2.21432     1.3692     0.9914   0.14271\n",
      "   9    'a'    0 [0.007, 0.009, 0.984]    0.5842   15.384    3.086    0.522     0.537       -0.403        2.028    5.91148     0.6538     0.3691   0.08108\n",
      "  10    'r'   17 [0.007, 0.009, 0.984]    0.2704    5.507    2.641    1.294     1.308       -1.160       -0.236    0.85264     1.1512     1.2160   0.00420\n",
      "  11    'i'    8 [0.007, 0.009, 0.984]    0.3197    8.558    2.653    1.126     1.140       -1.071       -2.736    2.77311     1.2241     1.4393   0.04630\n",
      "  12    'o'   14 [0.007, 0.009, 0.984]    0.0761    5.657    3.318    2.565     2.576        0.044       -1.029    1.14942     0.8108     1.1168   0.09363\n",
      "  13    'n'   13 [0.007, 0.009, 0.984]    0.4312    3.349    2.397    0.828     0.841        1.818        2.269    0.20374     1.2699     1.2216   0.00234\n",
      "  14  '\\\\n'   26 [0.007, 0.009, 0.984]    0.5980    2.086    3.929    0.500     0.514        0.329        0.475    0.02128     1.1509     1.6640   0.26328\n",
      "  15    'm'   12 [0.007, 0.009, 0.984]    0.0681    9.015    3.244    2.676     2.687       -1.394       -2.919    2.32298     1.3645     0.9914   0.13917\n",
      "  16    'y'   24 [0.007, 0.009, 0.984]    0.0322    5.611    4.260    3.424     3.435       -0.330       -1.421    1.19042     0.6518     1.2143   0.31640\n",
      "  17    'r'   17 [0.007, 0.009, 0.984]    0.1580    2.287    3.233    1.836     1.845        1.330        0.979    0.12271     1.0993     1.5024   0.16251\n",
      "  18    'a'    0 [0.007, 0.009, 0.984]    0.3327   13.341    2.325    1.087     1.101       -0.949        2.905   14.85519     1.1492     1.2160   0.00445\n",
      "  19    'n'   13 [0.007, 0.009, 0.984]    0.2292    4.173    2.674    1.460     1.473       -1.275       -1.952    0.45849     1.0288     0.9404   0.00781\n",
      "  20    'i'    8 [0.007, 0.009, 0.984]    0.1240    2.046    4.673    2.079     2.088        0.190        0.100    0.00813     0.8255     0.2102   0.37859\n",
      "  21    'l'   11 [0.007, 0.009, 0.984]    0.0626   10.932    4.292    2.756     2.770        0.101       -1.647    3.05648     0.8541     0.3177   0.28773\n",
      "  22    'e'    4 [0.007, 0.009, 0.984]    0.2289    2.045    2.658    1.465     1.474        0.560        0.836    0.07573     0.9941     0.9214   0.00529\n",
      "  23    'n'   13 [0.007, 0.009, 0.984]    0.1321   13.755    2.894    2.012     2.024       -0.378       -2.692    5.35482     0.7496     0.8916   0.02015\n",
      "  24  '\\\\n'   26 [0.007, 0.009, 0.984]    0.5934    2.505    4.102    0.507     0.522        0.117        0.475    0.12851     1.1243     1.6640   0.29123\n",
      "  25    'a'    0 [0.007, 0.009, 0.984]    0.1217   11.219    2.777    2.095     2.106       -1.452        3.043   20.20467     1.3553     1.1354   0.04835\n",
      "  26    'm'   12 [0.007, 0.009, 0.984]    0.1056    2.235    4.529    2.240     2.248       -0.990       -1.113    0.01509     0.9849     0.3691   0.37924\n",
      "  27    'e'    4 [0.007, 0.009, 0.984]    0.1540    1.632    3.660    1.865     1.871       -0.692       -0.489    0.04153     0.7481     0.3285   0.17600\n",
      "  28    'r'   17 [0.007, 0.009, 0.984]    0.3373    3.501    2.756    1.073     1.087       -0.614        0.269    0.77866     0.7015     0.7561   0.00299\n",
      "  29    'a'    0 [0.007, 0.009, 0.984]    0.1223   13.337    2.401    2.092     2.101       -0.954        2.905   14.89233     1.2108     1.2160   0.00003\n",
      "  30  '\\\\n'   26 [0.007, 0.009, 0.984]    0.3402    6.708    2.659    1.064     1.078       -1.196       -0.099    1.20347     1.0628     1.1354   0.00527\n",
      "  31    'm'   12 [0.007, 0.009, 0.984]    0.0723    9.287    2.986    2.617     2.626       -1.350       -2.919    2.45983     1.3124     0.9914   0.10306\n",
      "  32    'a'    0 [0.007, 0.009, 0.984]    0.5336   16.182    3.102    0.613     0.628       -0.559        2.028    6.69396     0.6577     0.3691   0.08330\n",
      "  33    'r'   17 [0.007, 0.009, 0.984]    0.2704    5.507    2.641    1.294     1.308       -1.160       -0.236    0.85264     1.1512     1.2160   0.00420\n",
      "  34    'i'    8 [0.007, 0.009, 0.984]    0.3197    8.558    2.653    1.126     1.140       -1.071       -2.736    2.77311     1.2241     1.4393   0.04630\n",
      "  35    'e'    4 [0.007, 0.009, 0.984]    0.1515    2.500    2.914    1.878     1.887        0.044        0.553    0.25971     0.8108     0.6978   0.01277\n",
      "  36  '\\\\n'   26 [0.007, 0.009, 0.984]    0.4640    7.654    2.838    0.753     0.768       -0.890        0.504    1.94361     0.7968     0.7730   0.00057\n",
      "  37    'j'    9 [0.007, 0.009, 0.984]    0.0783    4.550    2.795    2.539     2.547       -1.330       -2.270    0.88347     1.3389     1.5773   0.05683\n",
      "  38    'i'    8 [0.007, 0.009, 0.984]    0.0397    4.736    3.331    3.220     3.227        1.151        2.071    0.84649     0.9306     0.5323   0.15862\n",
      "  39    'e'    4 [0.007, 0.009, 0.984]    0.0718    4.779    2.827    2.626     2.634       -0.379        0.553    0.86915     0.7336     0.6978   0.00128\n",
      "  40    'l'   11 [0.007, 0.009, 0.984]    0.2725   11.059    2.931    1.286     1.300       -0.389       -2.306    3.67486     0.7635     0.9214   0.02492\n",
      "  41    'a'    0 [0.007, 0.009, 0.984]    0.0853    4.744    2.771    2.454     2.462        0.534        1.431    0.80471     1.0083     1.1803   0.02957\n",
      "  42  '\\\\n'   26 [0.007, 0.009, 0.984]    0.2587    5.596    2.635    1.339     1.352       -1.050       -0.099    0.90607     1.0999     1.1354   0.00126\n",
      "  43    'k'   10 [0.007, 0.009, 0.984]    0.1024    2.763    2.992    2.272     2.279       -1.379       -1.980    0.36170     1.3340     1.6467   0.09779\n",
      "  44    'e'    4 [0.007, 0.009, 0.984]    0.2197    2.243    2.472    1.506     1.516        1.711        1.590    0.01466     1.2048     1.1374   0.00455\n",
      "  45    'r'   17 [0.007, 0.009, 0.984]    0.1249    6.633    2.805    2.068     2.080       -0.975        0.269    1.54538     0.7615     0.7561   0.00003\n",
      "  46    'e'    4 [0.007, 0.009, 0.984]    0.0994   10.457    2.837    2.298     2.309       -0.772       -2.873    4.41380     1.1051     0.7561   0.12174\n",
      "  47    'n'   13 [0.007, 0.009, 0.984]    0.1806   12.422    2.896    1.699     1.712       -0.622       -2.692    4.28207     0.7446     0.8916   0.02161\n",
      "  48    'a'    0 [0.007, 0.009, 0.984]    0.0773    6.613    2.813    2.552     2.560        0.052        1.190    1.29414     0.8724     0.9404   0.00462\n",
      "  49  '\\\\n'   26 [0.007, 0.009, 0.984]    0.6080    4.802    2.623    0.483     0.498       -0.942       -0.099    0.71138     1.1329     1.1354   0.00001\n",
      "  50    'k'   10 [0.007, 0.009, 0.984]    0.0943    2.768    2.973    2.355     2.362       -1.377       -1.980    0.36406     1.3465     1.6467   0.09009\n",
      "  51    'y'   24 [0.007, 0.009, 0.984]    0.0561    4.742    7.921    2.865     2.880        1.711        2.533    0.67663     1.2048     0.1588   1.09421\n",
      "  52    'n'   13 [0.007, 0.009, 0.984]    0.2042    4.653    2.497    1.577     1.589        1.515        2.382    0.75145     1.0601     0.9569   0.01065\n",
      "  53  '\\\\n'   26 [0.007, 0.009, 0.984]    0.3075    2.297    4.350    1.166     1.179        0.209        0.475    0.07097     1.0872     1.6640   0.33260\n",
      "  54    'm'   12 [0.007, 0.009, 0.984]    0.0695    8.933    3.223    2.655     2.666       -1.408       -2.919    2.28270     1.3605     0.9914   0.13624\n",
      "  55    'a'    0 [0.007, 0.009, 0.984]    0.5654   14.972    3.077    0.555     0.570       -0.330        2.028    5.56166     0.6518     0.3691   0.07992\n",
      "  56    'i'    8 [0.007, 0.009, 0.984]    0.0712    3.617    3.034    2.635     2.642       -1.160       -1.734    0.33028     1.1512     0.8634   0.08279\n",
      "  57  '\\\\n'   26 [0.007, 0.009, 0.984]    0.1184    1.991    5.270    2.126     2.134        0.209        0.527    0.10147     0.7731     1.4704   0.48624\n",
      "  58    'j'    9 [0.007, 0.009, 0.984]    0.0792    4.462    2.778    2.528     2.536       -1.349       -2.270    0.84738     1.3599     1.5773   0.04727\n",
      "  59    'a'    0 [0.007, 0.009, 0.984]    0.5493    2.369    3.335    0.585     0.599        1.203        1.658    0.20626     0.9259     1.3240   0.15847\n",
      "  60    'm'   12 [0.007, 0.009, 0.984]    0.0860    2.706    5.488    2.443     2.453       -1.366       -1.113    0.06414     1.1258     0.3691   0.57267\n",
      "  61    'a'    0 [0.007, 0.009, 0.984]    0.2340   16.429    3.109    1.438     1.453       -0.613        2.028    6.97500     0.6594     0.3691   0.08431\n",
      "  62    'r'   17 [0.007, 0.009, 0.984]    0.3391    4.414    2.679    1.067     1.081       -1.001       -0.236    0.58473     1.1136     1.2160   0.01047\n",
      "  63  '\\\\n'   26 [0.007, 0.009, 0.984]    0.0799   15.634    7.866    2.510     2.526       -1.071        1.864    8.61783     1.2241     0.1806   1.08895\n",
      "  64    'm'   12 [0.007, 0.009, 0.984]    0.0824    9.260    3.519    2.483     2.496       -1.355       -2.919    2.44597     1.4163     0.9914   0.18050\n",
      "  65    'e'    4 [0.007, 0.009, 0.984]    0.1182    1.896    3.218    2.131     2.135       -0.787       -0.489    0.08888     0.6558     0.3285   0.10712\n",
      "  66    'r'   17 [0.007, 0.009, 0.984]    0.3356    4.592    2.835    1.078     1.092       -0.747        0.269    1.03147     0.7900     0.7561   0.00115\n",
      "  67    'a'    0 [0.007, 0.009, 0.984]    0.1223   13.337    2.401    2.092     2.101       -0.954        2.905   14.89233     1.2108     1.2160   0.00003\n",
      "  68  '\\\\n'   26 [0.007, 0.009, 0.984]    0.3402    6.708    2.659    1.064     1.078       -1.196       -0.099    1.20347     1.0628     1.1354   0.00527\n",
      "  69    'd'    3 [0.007, 0.009, 0.984]    0.0710   10.803    3.489    2.633     2.645       -1.350       -3.109    3.09385     1.3124     1.7638   0.20368\n",
      "  70    'a'    0 [0.007, 0.009, 0.984]    0.3810    4.112    2.999    0.951     0.965       -0.431        0.261    0.47951     1.0401     0.6551   0.14821\n",
      "  71    'n'   13 [0.007, 0.009, 0.984]    0.1023    4.071    2.778    2.270     2.280       -1.298       -1.952    0.42783     1.1141     0.9404   0.03017\n",
      "  72  '\\\\n'   26 [0.007, 0.009, 0.984]    0.2892    2.345    5.026    1.227     1.241        0.186        0.475    0.08356     0.9912     1.6640   0.45260\n",
      "  73    'm'   12 [0.007, 0.009, 0.984]    0.0727    8.919    3.238    2.610     2.621       -1.410       -2.919    2.27557     1.3633     0.9914   0.13831\n",
      "  74    'a'    0 [0.007, 0.009, 0.984]    0.5654   14.972    3.077    0.555     0.570       -0.330        2.028    5.56166     0.6518     0.3691   0.07992\n",
      "  75    'r'   17 [0.007, 0.009, 0.984]    0.2704    5.507    2.641    1.294     1.308       -1.160       -0.236    0.85264     1.1512     1.2160   0.00420\n",
      "  76    'e'    4 [0.007, 0.009, 0.984]    0.0673    9.635    3.516    2.686     2.698       -1.071       -2.873    3.24656     1.2241     0.7561   0.21900\n",
      "  77    'e'    4 [0.007, 0.009, 0.984]    0.1216    8.479    5.603    2.091     2.107       -0.543        0.000    0.29491     0.7497     0.0000   0.56200\n",
      "  78    'n'   13 [0.007, 0.009, 0.984]    0.1262   11.374    2.886    2.058     2.070       -0.809       -2.692    3.54544     0.7723     0.8916   0.01422\n",
      "  79  '\\\\n'   26 [0.007, 0.009, 0.984]    0.5201    2.454    4.783    0.639     0.654        0.138        0.475    0.11397     1.0249     1.6640   0.40834\n",
      "  80    'j'    9 [0.007, 0.009, 0.984]    0.0870    4.024    2.781    2.433     2.441       -1.452       -2.270    0.66913     1.3553     1.5773   0.04926\n",
      "  81    'e'    4 [0.007, 0.009, 0.984]    0.1470    1.571    2.529    1.916     1.918        1.174        1.185    0.00012     0.9511     0.8997   0.00264\n",
      "  82    'r'   17 [0.007, 0.009, 0.984]    0.1230    6.758    2.837    2.084     2.095       -0.988        0.269    1.57908     0.7916     0.7561   0.00126\n",
      "  83    'e'    4 [0.007, 0.009, 0.984]    0.1106   10.361    3.068    2.189     2.201       -0.827       -2.873    4.18540     1.1480     0.7561   0.15354\n",
      "  84  '\\\\n'   26 [0.007, 0.009, 0.984]    0.2161    5.161    2.792    1.519     1.532       -0.622        0.504    1.26954     0.7446     0.7730   0.00081\n",
      "  85    'k'   10 [0.007, 0.009, 0.984]    0.1019    2.764    3.002    2.276     2.283       -1.378       -1.980    0.36229     1.3281     1.6467   0.10146\n",
      "  86    'o'   14 [0.007, 0.009, 0.984]    0.0716   17.655    7.611    2.620     2.636        1.739       -1.777   12.36217     1.2061     0.1901   1.03234\n",
      "  87    'r'   17 [0.007, 0.009, 0.984]    0.2341    5.065    3.428    1.437     1.452        1.922        1.115    0.65071     1.2416     1.6969   0.20731\n",
      "  88    'r'   17 [0.007, 0.009, 0.984]    0.0734    6.923    8.082    2.596     2.612       -0.871        0.000    0.75932     1.0851     0.0000   1.17751\n",
      "  89    'i'    8 [0.007, 0.009, 0.984]    0.3566    9.582    2.643    1.017     1.031       -0.714       -2.736    4.09010     1.2414     1.4393   0.03914\n",
      "  90    'n'   13 [0.007, 0.009, 0.984]    0.1232   17.317    4.473    2.079     2.094       -0.070       -3.041    8.82881     0.7814     0.2102   0.32624\n",
      "  91  '\\\\n'   26 [0.007, 0.009, 0.984]    0.3962    3.921    5.876    0.910     0.926       -0.261        0.475    0.54166     0.8783     1.6640   0.61724\n",
      "  92    'k'   10 [0.007, 0.009, 0.984]    0.1023    2.667    3.012    2.272     2.279       -1.416       -1.980    0.31839     1.3224     1.6467   0.10515\n",
      "  93    'a'    0 [0.007, 0.009, 0.984]    0.4402    2.315    3.383    0.807     0.820        1.639        1.855    0.04670     1.2533     1.6904   0.19113\n",
      "  94    'i'    8 [0.007, 0.009, 0.984]    0.1373    3.048    2.918    1.976     1.986       -1.342       -1.734    0.15393     1.1042     0.8634   0.05795\n",
      "  95    'r'   17 [0.007, 0.009, 0.984]    0.1373    1.878    5.347    1.977     1.985        0.106        0.405    0.08953     0.7280     1.4393   0.50599\n",
      "  96    'i'    8 [0.007, 0.009, 0.984]    0.1566    8.762    2.684    1.842     1.854       -1.019       -2.736    2.95003     1.1847     1.4393   0.06479\n",
      "  97    'e'    4 [0.007, 0.009, 0.984]    0.1624    2.172    2.934    1.810     1.818        0.159        0.553    0.15561     0.8260     0.6978   0.01643\n",
      "  98  '\\\\n'   26 [0.007, 0.009, 0.984]    0.4640    7.654    2.838    0.753     0.768       -0.890        0.504    1.94361     0.7968     0.7730   0.00057\n",
      "  99    'r'   17 [0.007, 0.009, 0.984]    0.0553    1.280    9.219    2.913     2.895       -1.330       -1.277    0.00279     1.3389     0.1806   1.34164\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Averages over 100 generated steps:\n",
      " avg combined CE (nll): 1.7923\n",
      " avg ch1 CE: 6.5686, ch2 CE: 3.5574, ch3 CE: 1.7809\n",
      " avg ang MSE: 2.453219, avg dist MSE: 0.189602, avg entropy: 2.3428\n",
      "\n",
      "--- Generated sequence (context + generated) ---\n",
      "\n",
      "lynn\\nameri\\nmarion\\nmyranilen\\namera\\nmarie\\njiela\\nkerena\\nkyn\\nmai\\njamar\\nmera\\ndan\\nmareen\\njere\\nkorrin\\nkairie\\nr\n"
     ]
    }
   ],
   "source": [
    "# Corrected Detailed sampling loop: 100 tokens with losses & diagnostics per-step\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "# sanity checks\n",
    "assert 'model' in globals(), \"model not found. Run training cell first.\"\n",
    "assert 'coords' in globals(), \"coords tensor not found.\"\n",
    "assert 'VOCAB' in globals(), \"VOCAB not found.\"\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "def neg_log_prob_of_index(probs_tensor, idx):\n",
    "    p = float(probs_tensor[int(idx)].item())\n",
    "    return -np.log(max(p, EPS))\n",
    "\n",
    "def entropy_of_probs(probs_tensor):\n",
    "    p = probs_tensor.cpu().numpy()\n",
    "    p = np.clip(p, EPS, 1.0)\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "# Prepare a start context (reuse previous logic)\n",
    "if 'train_ds' in globals() and len(train_ds) > 0:\n",
    "    start_ctx = list(train_ds.windows[0][:3])\n",
    "else:\n",
    "    start_ctx = [ord('t')-ord('a'), ord('h')-ord('a'), ord('e')-ord('a')]\n",
    "\n",
    "# change na lyn as start prompt\n",
    "start_ctx = list([11, 24, 13])\n",
    "\n",
    "print(\"Start context (tokens):\", [VOCAB[i] if i < 26 else '\\\\n' for i in start_ctx])\n",
    "\n",
    "# We'll generate 100 tokens\n",
    "N_GEN = 100\n",
    "temperature = 1.0\n",
    "top_k = 5\n",
    "top_p = None\n",
    "seed = 42\n",
    "\n",
    "# initialize RNG for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "generated = list(start_ctx)  # contains context + generated tokens\n",
    "running = {\n",
    "    \"total_ce\": 0.0,\n",
    "    \"ch1_ce\": 0.0,\n",
    "    \"ch2_ce\": 0.0,\n",
    "    \"ch3_ce\": 0.0,\n",
    "    \"ang_mse\": 0.0,\n",
    "    \"dist_mse\": 0.0,\n",
    "    \"entropy\": 0.0\n",
    "}\n",
    "\n",
    "print(\"\\nDetailed per-step diagnostics (first 100 steps):\")\n",
    "print(\"-\" * 120)\n",
    "header = (\"step\", \"chosen\", \"idx\", \"ch_weights\", \"comb_prob\", \"ch1_ce\", \"ch2_ce\", \"ch3_ce\",\n",
    "          \"comb_ce\", \"ang_pred(rad)\", \"ang_tgt(rad)\", \"ang_sqerr\", \"dist_pred\", \"dist_tgt\", \"dist_sqerr\", \"entropy\")\n",
    "print(\"{:>4s} {:>6s} {:>4s} {:>18s} {:>9s} {:>8s} {:>8s} {:>8s} {:>9s} {:>12s} {:>12s} {:>10s} {:>10s} {:>10s} {:>9s}\".format(*header))\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for step in range(N_GEN):\n",
    "    t0, t1, t2 = generated[-3], generated[-2], generated[-1]\n",
    "    # get model outputs & probabilities using sample_next_token helper\n",
    "    next_idx, diag = sample_next_token(model, t0, t1, t2,\n",
    "                                       temperature=temperature, top_k=top_k, top_p=top_p,\n",
    "                                       deterministic=False)\n",
    "    # store token\n",
    "    generated.append(next_idx)\n",
    "\n",
    "    # retrieve per-channel probability vectors (torch tensors)\n",
    "    dev = device\n",
    "    with torch.no_grad():\n",
    "        t0t = torch.tensor([t0], dtype=torch.long, device=dev)\n",
    "        t1t = torch.tensor([t1], dtype=torch.long, device=dev)\n",
    "        t2t = torch.tensor([t2], dtype=torch.long, device=dev)\n",
    "        out = model(t0t, t1t, t2t)\n",
    "        p1 = F.softmax(out[\"ch1_logits\"].squeeze(0) / max(1e-8, temperature), dim=-1)  # tensor (V,)\n",
    "        p2 = F.softmax(out[\"ch2_logits\"].squeeze(0) / max(1e-8, temperature), dim=-1)\n",
    "        p3 = F.softmax(out[\"ch3_logits\"].squeeze(0) / max(1e-8, temperature), dim=-1)\n",
    "        w = out[\"channel_weights\"]  # tensor (3,)\n",
    "        combined = (w[0]*p1 + w[1]*p2 + w[2]*p3).cpu()\n",
    "\n",
    "        ang_pred = float(out[\"ang_pred\"].cpu().numpy()[0])\n",
    "        dist_pred = float(out[\"dist_pred\"].cpu().numpy()[0])\n",
    "\n",
    "    # compute negative-log-likelihood (CE) for the chosen token per channel\n",
    "    ch1_ce = neg_log_prob_of_index(p1.cpu(), next_idx)\n",
    "    ch2_ce = neg_log_prob_of_index(p2.cpu(), next_idx)\n",
    "    ch3_ce = neg_log_prob_of_index(p3.cpu(), next_idx)\n",
    "    comb_ce = neg_log_prob_of_index(combined, next_idx)\n",
    "    comb_prob_of_chosen = float(combined[int(next_idx)].item())\n",
    "\n",
    "    # regression targets: angle & distance from c2 to coords[next_idx]\n",
    "    # --- FIX: compute dy, dx as floats then use math.atan2 (avoids mixing tensor & float in torch.atan2)\n",
    "    c2 = coords[t2]            # tensor on CPU (coords is CPU); no .item() yet\n",
    "    c_next = coords[int(next_idx)]\n",
    "    dy = float((c_next - c2)[1].item())\n",
    "    dx = float((c_next - c2)[0].item())\n",
    "    ang_tgt = math.atan2(dy, dx)     # returns float\n",
    "    dist_tgt = float(torch.norm(c_next - c2).item())\n",
    "\n",
    "    ang_sqerr = (ang_pred - ang_tgt) ** 2\n",
    "    dist_sqerr = (dist_pred - dist_tgt) ** 2\n",
    "\n",
    "    # entropy of combined distribution\n",
    "    ent = entropy_of_probs(combined)\n",
    "\n",
    "    # accumulate running sums\n",
    "    running[\"total_ce\"] += comb_ce\n",
    "    running[\"ch1_ce\"] += ch1_ce\n",
    "    running[\"ch2_ce\"] += ch2_ce\n",
    "    running[\"ch3_ce\"] += ch3_ce\n",
    "    running[\"ang_mse\"] += ang_sqerr\n",
    "    running[\"dist_mse\"] += dist_sqerr\n",
    "    running[\"entropy\"] += ent\n",
    "\n",
    "    # pretty prints\n",
    "    chosen_display = '\\\\n' if int(next_idx) == 26 else VOCAB[int(next_idx)]\n",
    "    w_np = [round(float(x), 3) for x in w.cpu().numpy()]\n",
    "\n",
    "    print(f\"{step:4d} {repr(chosen_display):>6s} {next_idx:4d} {str(w_np):>18s} {comb_prob_of_chosen:9.4f} \"\n",
    "          f\"{ch1_ce:8.3f} {ch2_ce:8.3f} {ch3_ce:8.3f} {comb_ce:9.3f} \"\n",
    "          f\"{ang_pred:12.3f} {ang_tgt:12.3f} {ang_sqerr:10.5f} {dist_pred:10.4f} {dist_tgt:10.4f} {dist_sqerr:9.5f}\")\n",
    "\n",
    "# After loop show averages\n",
    "n = N_GEN\n",
    "print(\"-\" * 120)\n",
    "print(\"Averages over {} generated steps:\".format(n))\n",
    "print(\" avg combined CE (nll): {:.4f}\".format(running[\"total_ce\"]/n))\n",
    "print(\" avg ch1 CE: {:.4f}, ch2 CE: {:.4f}, ch3 CE: {:.4f}\".format(\n",
    "    running[\"ch1_ce\"]/n, running[\"ch2_ce\"]/n, running[\"ch3_ce\"]/n))\n",
    "print(\" avg ang MSE: {:.6f}, avg dist MSE: {:.6f}, avg entropy: {:.4f}\".format(\n",
    "    running[\"ang_mse\"]/n, running[\"dist_mse\"]/n, running[\"entropy\"]/n))\n",
    "\n",
    "# Optionally print the generated token string (mapping EOT -> newline)\n",
    "display_string = ''.join((('\\\\n' if int(i)==26 else VOCAB[int(i)]) for i in generated))\n",
    "print(\"\\n--- Generated sequence (context + generated) ---\\n\")\n",
    "print(display_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a07786-b4c5-490c-b819-6f03894af0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
